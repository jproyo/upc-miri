\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{titling} 
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{lastpage}
\title{Home work 3 - Solutions}
\author{Juan Pablo Royo Sales}
\date\today

\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{}
\fancyhead[R]{Juan Pablo Royo Sales - UPC MIRI}
\fancyhead[L]{RA - Homework 3}
\fancyfoot[L,C]{}
\fancyfoot[R]{Page \thepage{} of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\begin{document}

\begin{titlingpage}
  \maketitle
\end{titlingpage}

\section{Exercise 15}
\subsection{Part a}

Taking the solution of the \textbf{Problem 5} we can think that each $s_i$ is
generated by tossing a sequence of labeled coins with $(m_1,m_2....m_n)$ such as
it is state in the reference Problem.

Lets state that each time we obtain \textbf{Head} we toss again the same coin.

Knowing from \textbf{Problem 5} that:

\begin{equation}
  \frac{1}{r}\prod_{j=r+1}^{n} (1 - \frac{1}{j}) = \frac{1}{n}
\end{equation}

Taking into consideration the previous statement, we can see that repeating the
tossing when Head appears leads to $m$ times of tossing if Head appears those
$m$ times.

Rewriting the previous equation with this change we have:

\begin{equation}
  \left(\frac{1}{r}\right)^m\prod_{j=r+1}^{n} (1 - \frac{1}{j}) 
\end{equation}

Rewriting in terms of $j$ since $\frac{1}{r}$ is indicating the Probability of
Head, we can write that term as $\frac{1}{j}$ and knowing that $j$ starts at $2$
because we are going to reach 1 always by the algorithm, therefore:

\begin{subequations}
  \begin{align}
    P[\text{Getting speifici }s_i] &= \left(\frac{1}{j}\right)^m\prod_{j=2}^{n} (1 - \frac{1}{j}) \\
                                   &= \prod_{j=2}^{n} \left(\frac{1}{j}\right)^{m_j} (1 - \frac{1}{j}) 
  \end{align}
\end{subequations}

\subsection{Part b}
From the previous section, we have seen that the Probability of some $j$ integer
occurs in $s_i$ is $\left( \frac{1}{j} \right)^k\left( 1 - \frac{1}{j} \right)$.

Let $P$ be the set of primes $p \leq n \mid p \in P$

Let $Q$ be the subset of primes which are in the factorization of $r = \prod_{q
  \in Q} q^{m_q}$

\begin{equation}\label{eq:1}
  P[r \text{ is generated}] = \frac{1}{q} \mid q \in Q
\end{equation}

We know by the definition that $\alpha_n = \prod_p \left( 1 -
  \frac{1}{p}\right)$

Therefore taking \ref{eq:1} many times $m$ we can conclude

\begin{equation}
  \prod_{q \in Q} \left( \frac{1}{q} \right)^mq \prod_{p \in P} \left( 1 - \frac{1}{p} \right) = \frac{\alpha_n}{n}
\end{equation}

\subsection{Part c}
By the previous subsection we know that the probability of each $r \mid 1 \leq r
\leq n$ is $\frac{\alpha_n}{n}$

The probability of $r$ in one event is $\alpha_n$

Since we want to know the $E[X]$ being $X$ the number of repetitions needed
until we get $r$, this is a $G(p)$ Geometric distribution on $p$. Therefore the
expected value of a Geometric distribution is $\frac{1}{p}$. We can conclude
that the expected value until we get $r$ is $\frac{1}{\alpha_n} \sim 1.8\ln n$ 

\subsection{Part d}
The probability of primality test on each $j$ is equal to the probability of
$m_j$. We have shown from previous sections that it is $\frac{1}{j}$.

Taking this we can calculate the expected value defining an indicator random
variable

\[
\begin{cases}
  1 \text{ if primality test over } j \text{ is true}\\
  0 \text{ otherwise}
\end{cases}
\]

\begin{equation}\label{eq:2}
  E[X] = \sum_{j=1}^n\frac{1}{j} = \ln n
\end{equation}

Taking \ref{eq:2} armonic serie 

\section{Exercise 16}
If $X = Rand(1,n)$ then $P[X] = \frac{1}{n}$

And $E[X]=\sum_{x=1}^n xP[X=x] = \sum_{x=1}^nx\frac{1}{n}$

And $E[X^2]=\sum_{x=1}^n x^2P[X=x] = \sum_{x=1}^nx^2\frac{1}{n}$

Therefore,

\begin{equation}
  Var[X] = \sum_{x=1}^n(x + x^2)\frac{1}{n}
\end{equation}

\section{Exercise 18}
By Variance definition we know that

\begin{subequations}
  \begin{align}
    Var[X] &= E[(X - \mu)^2]\\
           &= E\left[\left( \sum_{i=1}^n X_i - E\left[ \sum_{i=1}^n X_i \right]\right)^2\right]\\
           &= E\left[\sum_{i=1}^n\sum_{j=1}^n (X_i - E[X_i]) (X_j - E[X_j])\right]\\
           &= E\left[\sum_{i=1}^n (X_i - E[X_i]) + 2 \sum_{i<j} (X_i - E[X_i])(X_j - E[X_j])\right]\\
           &= \sum_{i=1}^n Var[X_i] + 2 \sum_{i<j} E[X_iX_j - E[X_i]X_j - X_iE[X_j] + E[X_i]E[X_j]] \label{eq:3}\\
           &= \sum_{i=1}^n Var[X_i]
  \end{align}
\end{subequations}

Taking that \ref{eq:3} is equal to $0$ by definition of problem that $E[X_iX_j]
= E[X_i]E[X_j]$

\begin{equation}
  E[X_iX_j - E[X_i]X_j - X_iE[X_j] + E[X_i]E[X_j]] = 2E[X_i]E[X_j] - 2E[Xi_]E[X_j] = 0
\end{equation}

\section{Exercise 19}
\subsection{Part a}
Let $A$ be one event with a bit random value $0$ or $1$
Let $B$ be pair $A$ event with a bit random value

The probability in both events that the value be $0$ or $1$ is equally likely
$\frac{1}{2}$

Therefore,

\begin{subequations}
  \begin{align}
    P[Y_i = 0] &= P[A = 0]P[B = 0] + P[A = 1]P[B = 1]\\
               &= \frac{1}{2}\frac{1}{2} + \frac{1}{2}\frac{1}{2}\\
               &= \frac{1}{2} 
  \end{align}
\end{subequations}

By definition of Probability $P[Y_i = 1] = 1 - P[Y_i = 0] = \frac{1}{2}$

\subsection{Part b}
Suppose that $Y_i$ are mutually independent. It should be probe that $P[Y_i=0
\cap Y_j = 0] = P[Y_i=0]P[Y_j=0]$

By previous section we know that $P[Y_i=0] = \frac{1}{2}$, therefore
$P[Y_i=0]P[Y_j=0]=\frac{1}{4}$ \label{eq:4}

Let see the other part $P[Y_i = 0 \cap Y_j = 0]$

\begin{subequations}
  \begin{align}
    P[Y_i = 0 \cap Y_j = 0] &= P[Y_j = 0 \mid Y_i = 0] P[Y_i = 0]\\
                            &= \frac{1}{4} \frac{1}{2}\\
                            &= \frac{1}{8} \label{eq:5}
  \end{align}
\end{subequations}

Since \ref{eq:4} is not equal to \ref{eq:5}, they are not mutually independent

\subsection{Part c}
Taking out the cases when $Y_i = 0$ which is going to give us terms with $0$ in
the Expectation summation, we can count only the $1$

Expectation equality only holds when $Y_i$ and $Y_j$ are independent. So we are
assuming that $P[Y_i = 1 \cap Y_j = 1] = P[Y_i = 1]P[Y_j = 1]$

\begin{subequations}
  \begin{align}
    E[Y_iY_j] &= 1 P[Y_i=1 \cap Y_j=1]\\
              &= \frac{1}{4} \label{eq:6}
 \end{align}
\end{subequations}


\begin{subequations}
  \begin{align}
    E[Y_i]E[Y_j] &= 1 P[Y_i=1] 1 P[Y_j=1]\\
                 &= \frac{1}{2}\frac{1}{2}\\
                 &= \frac{1}{4} \label{eq:7}
  \end{align}
\end{subequations}

\subsection{Part d}

\begin{subequations}
  \begin{align}
    Var[Y] &= E[(Y - \mu)^2]\\
           &= E[Y^2] - E[Y]^2\\
           &= \sum_{i=1}^n Y_i^2\frac{1}{2} - \left[ \sum_{i=1}^n Y_i\frac{1}{2} \right]^2\\
           &= \frac{n}{2} - \left( \frac{n}{2} \right)^2 \\
           &= \frac{n^2 + 2n}{4}
  \end{align}
\end{subequations}

\subsection{Part e}
\begin{subequations}
  \begin{align}
    P[\mid Y - E[Y] \mid \geq n] &\leq \frac{Var[Y]}{n^2}\\
                               &\leq \frac{\frac{n^2 + 2n}{4}}{n^2}\\
                               &\leq \frac{n^4 + 2n^3}{4} 
  \end{align}
\end{subequations}

\section{Exercise 20}
\subsection{Part a}

Let $Y = \sum_{i=1}^nX_i$ then we can write Chebyshev inequality like:
\begin{subequations}
  \begin{align}
    P[\mid Y - E[Y] \mid \geq \epsilon E[Y]] &\leq \frac{Var[Y]}{(\epsilon E[Y])^2}\\
                                            &= \frac{Var[\sum_{i=1}^t]}{(\epsilon t E[X])^2}\\
                                            &= \frac{t Var[X]}{t^2 \epsilon^2 E[X]^2}\\
                                            &= \frac{r^2}{t \epsilon^2}\label{eq:8}
  \end{align}      

\end{subequations}


\ref{eq:8} by definition on the problem statement.

Taking $t \geq \frac{r^2}{\epsilon^2 \delta}$ therefore

\begin{subequations}
  \begin{align}
    P\left[\sum_{i=1}^t\frac{X_i}{t} \leq (1 + \epsilon) E[X]\right] &= 1 - P\left[\sum_{i=1}^tX_i > t(1 + \epsilon) E[X]\right]\\ 
                                                                     &= 1 - \frac{r^2}{t \epsilon^2}\\
                                                                     &\geq 1 - \delta \label{eq:9}
  \end{align}      
\end{subequations}

Therefore the number of samples are $\frac{r^2}{\epsilon^2 \delta}$


\subsection{Part b}
Taking that probability is $\frac{3}{4}$ then $\delta = \frac{1}{4}$

As we state before that $O(r^2/\epsilon^2 \delta)$, replacing by probability
$\frac{1}{4}$, we have

\begin{equation}
  O(4r^2/\epsilon^2) = O(r^2/\epsilon^2)
\end{equation}

\subsection{Part c}
Let X be a random variable which satisfies the following condition:

\[
  X = \begin{cases}
    1 \text{ if the } nth \text{ week estimate is above } \epsilon E[X]\\
    0 \text{ otherwise}
  \end{cases}
\]

Using Chernoff's bound we have:

\begin{equation}
  P[X \geq (1 + \delta')E[X]] \leq e^{-E[X] \delta'^2 / 3}
\end{equation}

Taking $E[X] = \frac{m}{4}$ and $\delta' = 1$ then,

\begin{equation}
  P[X \geq m/2] \leq e^{-m / 2}
\end{equation}

If we take $m = 12 \log(1/ \delta)$, then $P[X \geq m/2] \leq \delta$

Therefore, the probability of the median weak for $\epsilon E[X]$ is at least $1
- \delta$

The process of weak estimate uses $O(r^2/ \epsilon^2)$ samples, and there are
$O(\log{1/ \delta})$ weak estimates, therefore the number of total samples are
in $O(r^2 \log(1 / \delta)/ \epsilon^2)$

\end{document}
