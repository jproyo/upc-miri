<!-- R Commander Markdown Template -->

SMDE - First Assignment
=======================

### Juan Pablo Royo Sales

### `r as.character(Sys.Date())`

```{r echo=FALSE}
# include this code chunk as-is to set options
knitr::opts_chunk$set(comment=NA, prompt=TRUE, out.width=750, fig.height=8, fig.width=8)
library(Rcmdr)
library(car)
library(RcmdrMisc)
```


```{r echo=FALSE}
# include this code chunk as-is to enable 3D graphs
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```

# Report and Code Organization

This assignment has been organized on the following way in order to find in a straightforward way all the resources needed.

## Folders

1. **src**: You can find here the Java code of RNG implemented.
2. **samples**: You can find here all the ***csv*** files that are being loaded by the Markdown script.
3. **report**: You can find here this markdown report alongside with the R script.

# Prerequistes

Although I have tried to reference all the path in the script and Markdown file with a relative path, I wasn't able to get it worked with relative path. I don't know if it is because of the version or the OS, which is an OSx, but they only way I was able to load ***csv*** files was with Absolute paths.

**You must need to change all the path in this file in order to run the script or Markdown**



# Question 1 - Assignment

## Loading data generated by Java Fib-Flagged method.

1. Load Data

```{r}
fibflagged <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/500_random_numbers.csv", 
    header = FALSE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE)
```

2. Apply bins

```{r}
fibflagged$bins <- with(fibflagged, binVariable(V1, bins = 10, method = "intervals", 
    labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")))
```

3. Generate Frequency

```{r}
fibflagged_trans <- as.data.frame(with(fibflagged, table(bins)))
```


## Generating uniform distribution with R

1. Load data

```{r}
distribution_r <- as.data.frame(matrix(runif(500 * 1, min = 0, max = 1), ncol = 1))
rownames(distribution_r) <- paste("sample", 1:500, sep = "")
colnames(distribution_r) <- "obs"
distribution_r <- within(distribution_r, {
    mean <- rowMeans(distribution_r[, 1:1])
})
```

2. Apply bins

```{r}
distribution_r$bins <- with(distribution_r, binVariable(obs, bins = 10, method = "intervals", 
    labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")))
```

3. Generate Frequency

```{r}
distribution_r_trans <- as.data.frame(with(distribution_r, table(frequency)))
```

## Merge both frequencies R and Fib-Flagged

```{r}
merge_fibflagged_r_comb <- merge(fibflagged_trans, distribution_r_trans, all = TRUE, 
    by = "row.names")
rownames(merge_fibflagged_r_comb) <- merge_fibflagged_r_comb$Row.names
merge_fibflagged_r_comb$Row.names <- NULL
```

## Generate table for Chi test

```{r}
to_be_test_chi_r_fibflagged <- within(merge_fibflagged_r_comb, {
	bins <- NULL
	frequency <- NULL
})
```

## Run Chi Test

```{r}
test <- chisq.test(to_be_test_chi_r_fibflagged, correct = FALSE)
```

### Conclusion Chi Test

After this **test** has a **p-value** of **0.819475004287325** , therefore with can deduce that we can accept the algorithm but for a number of 10 bins as it seems it is generating Random Numbers in an uniform distribution.


## Another test with 20 bins on each distribution

Now we are going to refrequence the bins to 20

```{r}
fibflagged$morebins <- with(fibflagged, binVariable(V1, bins = 20, method = "intervals", 
    labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
        "14", "15", "16", "17", "18", "19", "20")))
```


```{r}
distribution_r$morebins <- with(distribution_r, binVariable(obs, bins = 20, method = "intervals", 
    labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
        "14", "15", "16", "17", "18", "19", "20")))
```

## Count Frequency on 20 bins 

Count the frequency on each distribution

```{r}
fibflagged_trans_20bins <- as.data.frame(with(fibflagged, table(morebins)))
```

```{r}
distribution_r_trans_20bins <- as.data.frame(with(distribution_r, table(morebins)))
```

## Merge both frequency distribution with 20 bins


```{r}
merge_fibflagged_r_comb_20bins <- merge(fibflagged_trans_20bins, distribution_r_trans_20bins, 
    all = TRUE, by = "row.names")
```


```{r}
rownames(merge_fibflagged_r_comb_20bins) <- merge_fibflagged_r_comb_20bins$Row.names
```


```{r}
merge_fibflagged_r_comb_20bins$Row.names <- NULL
```

## Run Chi-square Test


```{r}
test_chi_r_fibflagged_20bins <- within(merge_fibflagged_r_comb_20bins, {
    morebins.x <- NULL
    morebins.y <- NULL
})
```


```{r}
chi_20bins <- chisq.test(test_chi_r_fibflagged_20bins, correct = FALSE)
```

### Conclusion Chi Test

After this **p-value** of **0.00175158290728414**, therefore with can deduce when we increase the beans the algorithm is not generating a random uniform distribution.


# Question 2 - Assignment


## Loading Normal distribution generated by code

1. Normal mu = 0, sigma = 1

```{r}
Norm_m0_s1 <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/1500_normal_mu_0_sigma_1.csv", 
    header = FALSE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE)
```

2. Normal mu = 10, sigma = 1

```{r}
Norm_m10_s1 <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/1500_normal_mu_10_sigma_1.csv", 
    header = FALSE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE)
```

3. Normal mu = 0, sigma = 1

```{r}
Norm_m0_s1_2 <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/1500_normal_mu_0_sigma_1_2.csv", 
    header = FALSE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE)
```


## Run Anova analysis

```{r}
Norm_v1n = data.frame(x1 = Norm_m0_s1, x2 = "v1")
```


```{r}
Norm_v2n = data.frame(x1 = Norm_m10_s1, x2 = "v2")
```


```{r}
Norm_v3n = data.frame(x1 = Norm_m0_s1_2, x2 = "v3")
```


```{r}
data = mergeRows(Norm_v1n, Norm_v2n, common.only = FALSE)
```


```{r}
data = mergeRows(as.data.frame(data), Norm_v3n, common.only = FALSE)
```

```{r}
AnovaModel.1 <- aov(V1 ~ x2, data = data)
```


```{r}
summary(AnovaModel.1)
```


```{r}
Boxplot(V1 ~ x2, data = data, id.method = "y")
```


### Conclusions Anova Analysis 

1. We already know beforehand that we have set 3 different populations which means are different, therefore the small p-value of **Pr(>F) <2e-16** and with a high **F value of 49071** which indicate rejection shows this.
2. For this part of the test we have proof our assumptions.


## Running Test for checking assumptions

```{r}
library("lmtest", lib.loc = "~/R/win-library/3.0")
```


```{r}
dwtest(AnovaModel.1, alternative = "two.sided")
```


```{r}
shapiro.test(residuals(AnovaModel.1))
```


```{r}
lmtest::bptest(AnovaModel.1)
```


### Conclusion Test assumptions


After running Durbin Watson, Shapiro and Brewusch-Pagan tests we can state the following conclusions:

1. It has been tried different number size of Normal Distribution, first with 500 each, after with 3000 but in this case Shapiro doesn't support such a big number, and finally because of the amount restrictions it has been tested with 1500 samples each Distribution.
2. Regarding Durbin-Watson test, it has been shown that the observations doesn't seem independents at all with a **p-value** of **0.127**
3. In all cases Shapiro Test gave us a really small **p-value** of 0.006966, which it means that the Distribution doesn't seem normal. 
4. Finally for Homogeneity of variance with the last test it has been found out that **p-value** is **1** which means it is homogeneous. 

It can be deduced based on Question 1, that the RNG it is not so random, because in Question 1 when we increase the bins, we obtained a rejection area.

## Anova on Wine FactoMinerR Dataset

First we load the Wine Dataset

```{r}
wine_data <- data(wine, package = "FactoMineR")
```

### Analysis of Soil regarding Odor Intensity before shaking

```{r}
wine_data_soil_odori <- wine[, c("Odor.Intensity.before.shaking", "Soil")]
colnames(wine_data_soil_odori) <- c("odori", "soil")
AnovaModel_soil_odori <- aov(odori ~ soil, data = wine_data_soil_odori)
summary(AnovaModel_soil_odori)
```

Running the previous Anova Model on **Soil** taking the **factor of Odor Intensity before shaking** we obtain a small p-value of **Pr(>F) 0.00397** and with a **F value of 6.496** indicating that we can reject the null hypothesis, therefore the populations are different.
This means that different Soils are affecting the Odor intensity before shaking factor.



### Analysis of Soil regarding Aroma Quality before shaking


```{r}
wine_data_soil_aromaq <- wine[, c("Aroma.quality.before.shaking", "Soil")]
colnames(wine_data_soil_aromaq) <- c("aromaq", "soil")
AnovaModel_soil_aromaq <- aov(aromaq ~ soil, data = wine_data_soil_aromaq)
summary(AnovaModel_soil_aromaq)
```

Running the previous Anova Model on **Soil** taking the **factor of Aroma Quality before shaking** we obtain a small p-value of **Pr(>F) 0.0248** and with a **F value of 4.023** indicating that we can reject the null hypothesis, therefore the populations are different.
This means that different Soils are affecting the Aroma quality of the wine before shaking.


### Analysis of Label regarding Odor Intensity before shaking

```{r}
wine_data_label_odori <- wine[, c("Odor.Intensity.before.shaking", "Label")]
colnames(wine_data_label_odori) <- c("odori", "label")
AnovaModel_label_odori <- aov(odori ~ label, data = wine_data_label_odori)
summary(AnovaModel_label_odori)
```

Running the previous Anova Model on **Label** taking the **factor of Odor Intensity before shaking** we obtain a small p-value of **Pr(>F) 0.0231** and with a **F value of 4.679** indicating that we can reject the null hypothesis, therefore the populations are different.
This means that different Labels are also affecting the Odor intensity before shaking factor.



### Analysis of Label regarding Aroma Quality before shaking


```{r}
wine_data_label_aromaq <- wine[, c("Aroma.quality.before.shaking", "Label")]
colnames(wine_data_label_aromaq) <- c("aromaq", "label")
AnovaModel_label_aromaq <- aov(aromaq ~ label, data = wine_data_label_aromaq)
summary(AnovaModel_label_aromaq)
```


Running the previous Anova Model on **Label** taking the **factor of Aroma Quality before shaking** we obtain a small p-value of **Pr(>F) 0.187** and with a **F value of 1.843** indicating that we can reject the null hypothesis, therefore the populations are different.
This means that different Labels are also affecting the Aroma quality of the wine before shaking.


### Conclusions ANOVA on wine FactoMineR

It seems that both are features Label and Soil are affecting wine factors. Although this, we can state from the analysis above that in the case of **Label** feature for Aroma Quality before shaking,
it is not affecting so much as the other because in spite the *p-value* is small it is near 0.20 which indicates that the populations are more similar in means than the other cases.


# Question 3 - Assignment

## Decathlon Data

### Loading data

```{r}
data(decathlon, package = "FactoMineR")
names(decathlon) <- make.names(names(decathlon))
```

### Splitting Data - Training and Test

```{r}
if (!require("caTools")) {
    install.packages("caTools")
    library(caTools)
}
split = sample.split(decathlon, SplitRatio = 0.6)
training_decathlon_set = subset(decathlon, split == TRUE)
test_decathlon_set = subset(decathlon, split == FALSE)
```

## Analyzing Data

### First Scenario of 1500m with all Variables

```{r}
names(training_decathlon_set) <- make.names(names(training_decathlon_set))
RegModel.17 <- lm(X1500m ~ Discus + High.jump + Javeline + Long.jump + Points + Pole.vault + 
    Rank + Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.17)
```

We can see based on the results that the rank is not affecting the model, so we can remove it.

```{r}
RegModel.18 <- lm(X1500m ~ Discus + High.jump + Javeline + Long.jump + Points + Pole.vault + 
    Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.18)
```

Here we have a very good model to predict the next results. We have an slope since the **p-value** has a small value **4.842e-14**

Lets try to remove Points to see what happen with the model


```{r}
RegModel.19 <- lm(X1500m ~ Discus + High.jump + Javeline + Long.jump + Pole.vault + 
    Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.19)
```

We can see that if we remove points, we dont have almost relation between the variables to explain the model. Lets add Points again and remove other Variable to see if the relations stay still.


```{r}
RegModel.21 <- lm(X1500m ~ Discus + High.jump + Long.jump + Points + Pole.vault + 
    Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.21)
```

We can see again if we keep Points and remove other like Javaline, we again couldn't have a model to do predictions. We are going to add again Javaline.

```{r}
RegModel.23 <- lm(X1500m ~ Discus + High.jump + Javeline + Long.jump + Points + Pole.vault + 
    Shot.put + X100m + X110m.hurdle + X400m, data = training_decathlon_set)
summary(RegModel.23)
```

### Testing assumptions on this linear model

1. Durbin Watson Test for independence

```{r}
library("lmtest", lib.loc = "~/R/win-library/3.0")
dwtest(RegModel.23, alternative = "two.sided")
```

We can see that the Regression Model pass the independence test with a p-value of **0.3621**

2. Shapiro Test for Normality 

```{r}
shapiro.test(residuals(RegModel.23))
```

We can see that the distribution is a Normal distribution with a p-value of **0.5407**

2. Brewusch Pagan Test - Homogeneity of the Variance


```{r}
bptest(RegModel.23)
```

We can see here that since the p-value is small **0.2054**, although not too much, the variance seems homogeneous.


### Predictions


```{r}
prediction <- predict(RegModel.23, newdata = test_decathlon_set, interval = "prediction")
prediction
```

Lets compare with the Real data

```{r}
test_only_with_x1500m <- subset(test_decathlon_set, select = c("X1500m"))
test_only_with_x1500m
```

```{r}
predicted_data <- data.frame(prediction)
predicted_data["real_data_x1500m"] = test_only_with_x1500m["X1500m"]
predicted_data
```

As we can appreciate all the real values of the test data set are between the interval **lwr** and **upr**, which indicates that the prediction was properly done, because the model with the trained data was accurate.

# Question 4 - Assignment

## PCA - Decathlon

```{r}
decathlon.PCA <- decathlon[, c("X100m", "Long.jump", "Shot.put", "High.jump", "X400m", 
    "X110m.hurdle", "Discus", "Pole.vault", "Javeline", "X1500m", "Rank", "Points")]
res <- PCA(decathlon.PCA, scale.unit = TRUE, ncp = 5, graph = FALSE)
plot.PCA(res, axes = c(1, 2), choix = "ind", habillage = "none", col.ind = "black", 
    col.ind.sup = "blue", col.quali = "magenta", label = c("ind", "ind.sup", "quali"), 
    new.plot = TRUE)
plot.PCA(res, axes = c(1, 2), choix = "var", new.plot = TRUE, col.var = "black", 
    col.quanti.sup = "blue", label = c("var", "quanti.sup"), lim.cos2.var = 0)
summary(res, nb.dec = 3, nbelements = 10, nbind = 10, ncp = 3, file = "")
remove(decathlon.PCA)
```

Taking into consideration this result we can state the following conclusions:

- **X110m hurdle**, **X100m** and **Rank** are correlated. There is also a correlation with **X400m** but not so strong as the others. Also there is a negative correlation between those and **Points**. 
This means that this type of competition which requires **strength** are related, which means that any athlete who has this quality is able to perform well in any of these trials.
- **Discuss**, **Javaline**, **Shot.put** are also correlated alongside with **High.jump**. Taking the same analysis as before we can see that these type of proves are related because all of them requires 
**agility** as quality to perform well on all of this.
- **Long.jump** and **Pole.vault** are not correlated with any of the others and it could be explaining that this kind of trials requires different capabilities such as agility and strength.
- Regarding **X1500m** which has been our point of analysis in the previous Question, we can see that also is not correlated with any other dimension. That explain why we need everything to do the prediction.


## Analysis Boston Marathon Results - 2017

For the analysis i am going to take Men between 30 and 40 years old. 

Lets first take a sample and perform an ANOVA analysis in order to check if the population is Normal.


### ANOVA Analysis


```{r}
boston_mar_men_30_40_training <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/boston_marathon_men_30_40_training.csv", 
    header = TRUE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE, quote = "\"")
boston_mar_men_18_30 <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/boston_marathon_men_18_30.csv", 
    header = TRUE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE, quote = "\"")
boston_mar_men_40_50 <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/boston_marathon_men_40_50.csv", 
    header = TRUE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE, quote = "\"")
```

```{r}
boston_mar_men_30_40_official_time = boston_mar_men_30_40_training[, c("Official.Time")]
boston_mar_men_18_30_official_time = boston_mar_men_18_30[, c("Official.Time")]
boston_mar_men_40_50_official_time = boston_mar_men_40_50[, c("Official.Time")]
Norm_boston_mar_30_40 = data.frame(x1 = boston_mar_men_30_40_official_time, x2 = "men_30_40")
Norm_boston_mar_30_40 = data.frame(x1 = boston_mar_men_30_40_official_time, x2 = "men_30_40")
Norm_boston_mar_18_30 = data.frame(x1 = boston_mar_men_18_30_official_time, x2 = "men_18_30")
Norm_boston_mar_40_50 = data.frame(x1 = boston_mar_men_40_50_official_time, x2 = "men_40_50")
boston_mar_merge = mergeRows(Norm_boston_mar_30_40, Norm_boston_mar_18_30, common.only = FALSE)
boston_mar_merge = mergeRows(as.data.frame(boston_mar_merge), Norm_boston_mar_40_50, 
    common.only = FALSE)
boston_mar_merge$x1 = as.difftime(as.character(boston_mar_merge$x1))
boston_mar_merge$x1 = as.numeric(boston_mar_merge$x1, units = "secs")
AnovaModel.Boston.Marathon.Merge <- aov(x1 ~ x2, data = boston_mar_merge)
```

Unfortunately as we can see the populations are different because **p-value** is too small. I have tried different ones but it seems obvious that the age is a predominant factor
in terms of performance and because of that the distributions between different ages for men have large variability, therefore ANOVA analysis indicates that does not belong to the same population.


```{r}
library("lmtest", lib.loc = "~/R/win-library/3.0")
dwtest(AnovaModel.Boston.Marathon.Merge, alternative = "two.sided")
lmtest::bptest(AnovaModel.Boston.Marathon.Merge)
```

Regarding Testing we can see that it doesn't pass the test neither of homoscedasticity, independence and homogeneity of the variance.

Although this we are going to try to solve the Linear regression Model.


### Linear Regression Model

```{r}
boston_mar_men_30_40_training$X5K = as.difftime(as.character(boston_mar_men_30_40_training$X5K))
boston_mar_men_30_40_training$X5K = as.numeric(boston_mar_men_30_40_training$X5K, 
    units = "secs")
boston_mar_men_30_40_training$X10K = as.difftime(as.character(boston_mar_men_30_40_training$X10K))
boston_mar_men_30_40_training$X10K = as.numeric(boston_mar_men_30_40_training$X10K, 
    units = "secs")
boston_mar_men_30_40_training$X15K = as.difftime(as.character(boston_mar_men_30_40_training$X15K))
boston_mar_men_30_40_training$X15K = as.numeric(boston_mar_men_30_40_training$X15K, 
    units = "secs")
boston_mar_men_30_40_training$X20K = as.difftime(as.character(boston_mar_men_30_40_training$X20K))
boston_mar_men_30_40_training$X20K = as.numeric(boston_mar_men_30_40_training$X20K, 
    units = "secs")
boston_mar_men_30_40_training$Half = as.difftime(as.character(boston_mar_men_30_40_training$Half))
boston_mar_men_30_40_training$Half = as.numeric(boston_mar_men_30_40_training$Half, 
    units = "secs")
boston_mar_men_30_40_training$X25K = as.difftime(as.character(boston_mar_men_30_40_training$X25K))
boston_mar_men_30_40_training$X25K = as.numeric(boston_mar_men_30_40_training$X25K, 
    units = "secs")
boston_mar_men_30_40_training$X30K = as.difftime(as.character(boston_mar_men_30_40_training$X30K))
boston_mar_men_30_40_training$X30K = as.numeric(boston_mar_men_30_40_training$X30K, 
    units = "secs")
boston_mar_men_30_40_training$X35K = as.difftime(as.character(boston_mar_men_30_40_training$X35K))
boston_mar_men_30_40_training$X35K = as.numeric(boston_mar_men_30_40_training$X35K, 
    units = "secs")
boston_mar_men_30_40_training$X40K = as.difftime(as.character(boston_mar_men_30_40_training$X40K))
boston_mar_men_30_40_training$X40K = as.numeric(boston_mar_men_30_40_training$X40K, 
    units = "secs")
boston_mar_men_30_40_training$Pace = as.difftime(as.character(boston_mar_men_30_40_training$Pace))
boston_mar_men_30_40_training$Pace = as.numeric(boston_mar_men_30_40_training$Pace, 
    units = "secs")
boston_mar_men_30_40_training$Official.Time = as.difftime(as.character(boston_mar_men_30_40_training$Official.Time))
boston_mar_men_30_40_training$Official.Time = as.numeric(boston_mar_men_30_40_training$Official.Time, 
    units = "secs")
```

```{r}
RegModel.boston_mar_men_30_40.1 <- lm(Official.Time ~ Age + Bib + Division + Gender + 
    Half + Overall + Pace + X + X5K + X10K + X15K + X20K + X25K + X30K + X35K + X40K, 
    data = boston_mar_men_30_40_training)
summary(RegModel.boston_mar_men_30_40.1)
RegModel.boston_mar_men_30_40.2 <- lm(Official.Time ~ X30K + X35K + X40K, data = boston_mar_men_30_40_training)
summary(RegModel.boston_mar_men_30_40.2)
RegModel.boston_mar_men_30_40.3 <- lm(Official.Time ~ X35K + X40K, data = boston_mar_men_30_40_training)
summary(RegModel.boston_mar_men_30_40.3)
```

According to these linear regression models, the best fit is to take into consideration the last 2 sprints in which there is a strong correlation with the final performance or the Official Time.
This assumption would seem right because as runners we know the difficulty of the last 10 km of a marathon.

Lets run the test of the Linear Regression selected model.


```{r}
library("lmtest", lib.loc = "~/R/win-library/3.0")
dwtest(RegModel.boston_mar_men_30_40.3, alternative = "two.sided")
shapiro.test(residuals(RegModel.boston_mar_men_30_40.3))
bptest(RegModel.boston_mar_men_30_40.3)
```

As we can see by the results, there are correlation which we already knew because of the big **R-squared**
 value obtained, but we cannot be sure about independence and normality.

### Prediction with Test Data

Lets try to predict with the test data to see if the model fits.



```{r}
boston_mar_men_30_40_test <- read.table("/Users/juan/Projects/upc/master/smde/smde-rng/boston_marathon_men_30_40_test.csv", 
    header = TRUE, sep = ",", na.strings = "NA", dec = ".", strip.white = TRUE, quote = "\"")
```


```{r}
boston_mar_men_30_40_test$X5K = as.difftime(as.character(boston_mar_men_30_40_test$X5K))
boston_mar_men_30_40_test$X5K = as.numeric(boston_mar_men_30_40_test$X5K, units = "secs")
boston_mar_men_30_40_test$X10K = as.difftime(as.character(boston_mar_men_30_40_test$X10K))
boston_mar_men_30_40_test$X10K = as.numeric(boston_mar_men_30_40_test$X10K, units = "secs")
boston_mar_men_30_40_test$X15K = as.difftime(as.character(boston_mar_men_30_40_test$X15K))
boston_mar_men_30_40_test$X15K = as.numeric(boston_mar_men_30_40_test$X15K, units = "secs")
boston_mar_men_30_40_test$X20K = as.difftime(as.character(boston_mar_men_30_40_test$X20K))
boston_mar_men_30_40_test$X20K = as.numeric(boston_mar_men_30_40_test$X20K, units = "secs")
boston_mar_men_30_40_test$Half = as.difftime(as.character(boston_mar_men_30_40_test$Half))
boston_mar_men_30_40_test$Half = as.numeric(boston_mar_men_30_40_test$Half, units = "secs")
boston_mar_men_30_40_test$X25K = as.difftime(as.character(boston_mar_men_30_40_test$X25K))
boston_mar_men_30_40_test$X25K = as.numeric(boston_mar_men_30_40_test$X25K, units = "secs")
boston_mar_men_30_40_test$X30K = as.difftime(as.character(boston_mar_men_30_40_test$X30K))
boston_mar_men_30_40_test$X30K = as.numeric(boston_mar_men_30_40_test$X30K, units = "secs")
boston_mar_men_30_40_test$X35K = as.difftime(as.character(boston_mar_men_30_40_test$X35K))
boston_mar_men_30_40_test$X35K = as.numeric(boston_mar_men_30_40_test$X35K, units = "secs")
boston_mar_men_30_40_test$X40K = as.difftime(as.character(boston_mar_men_30_40_test$X40K))
boston_mar_men_30_40_test$X40K = as.numeric(boston_mar_men_30_40_test$X40K, units = "secs")
boston_mar_men_30_40_test$Pace = as.difftime(as.character(boston_mar_men_30_40_test$Pace))
boston_mar_men_30_40_test$Pace = as.numeric(boston_mar_men_30_40_test$Pace, units = "secs")
boston_mar_men_30_40_test$Official.Time = as.difftime(as.character(boston_mar_men_30_40_test$Official.Time))
boston_mar_men_30_40_test$Official.Time = as.numeric(boston_mar_men_30_40_test$Official.Time, 
    units = "secs")
```

```{r}
prediction_boston_mar_men_30_40 <- predict(RegModel.boston_mar_men_30_40.3, newdata = boston_mar_men_30_40_test, 
    interval = "prediction")
# Showing only first 50
head(prediction_boston_mar_men_30_40, n=50)
```


```{r}
prediction_boston_mar_men_30_40_data <- data.frame(prediction_boston_mar_men_30_40)
# Showing only first 50
head(prediction_boston_mar_men_30_40_data, n=50)
```


```{r}
boston_mar_30_40_test_only_with_official_time <- subset(boston_mar_men_30_40_test, 
    select = c("Official.Time"))
# Showing only first 50
head(boston_mar_30_40_test_only_with_official_time, n=50)
```


```{r}
prediction_boston_mar_men_30_40_data["real_data_official_time"] = boston_mar_30_40_test_only_with_official_time["Official.Time"]
# Showing only first 50
head(prediction_boston_mar_men_30_40_data, n=50)
```

As we can see in some cases the real Official Time is not inside the interval predicted, but it is close enough to be a quite accurate prediction.

To finalize lets do a PCA (Principal Component Analysis) and check the results.

### PCA over Training Data

```{r}
boston_mar_men_30_40_training.PCA <- boston_mar_men_30_40_training[, c("X", "Bib", 
    "Age", "X5K", "X10K", "X15K", "X20K", "Half", "X25K", "X30K", "X35K", "X40K", 
    "Pace", "Official.Time", "Overall", "Gender", "Division")]
res <- PCA(boston_mar_men_30_40_training.PCA, scale.unit = TRUE, ncp = 5, graph = FALSE)
plot.PCA(res, axes = c(1, 2), choix = "ind", habillage = "none", col.ind = "black", 
    col.ind.sup = "blue", col.quali = "magenta", label = c("ind", "ind.sup", "quali"), 
    new.plot = TRUE)
plot.PCA(res, axes = c(1, 2), choix = "var", new.plot = TRUE, col.var = "black", 
    col.quanti.sup = "blue", label = c("var", "quanti.sup"), lim.cos2.var = 0)
summary(res, nb.dec = 3, nbelements = 10, nbind = 10, ncp = 3, file = "")
```

As we can see in the PCA, the model for prediction was selected wrongly because the most correlated variables are **30k and 35k** and we have chosen 35 and 40. 
Lets revisit and run again the Linear regression with that parameters.

### New Linear Regression with 30k and 35k

```{r}
RegModel.boston_mar_men_30_40.4 <- lm(Official.Time ~ X30K + X35K, data = boston_mar_men_30_40_training)
summary(RegModel.boston_mar_men_30_40.4)
library("lmtest", lib.loc = "~/R/win-library/3.0")
dwtest(RegModel.boston_mar_men_30_40.4, alternative = "two.sided")
shapiro.test(residuals(RegModel.boston_mar_men_30_40.4))
bptest(RegModel.boston_mar_men_30_40.4)
```


```{r}
prediction_boston_mar_men_30_40.2 <- predict(RegModel.boston_mar_men_30_40.4, newdata = boston_mar_men_30_40_test, 
    interval = "prediction")
prediction_boston_mar_men_30_40.2_data <- data.frame(prediction_boston_mar_men_30_40.2)
boston_mar_30_40_test_only_with_official_time.2 <- subset(boston_mar_men_30_40_test, 
    select = c("Official.Time"))
prediction_boston_mar_men_30_40.2_data["real_data_official_time"] = boston_mar_30_40_test_only_with_official_time.2["Official.Time"]
head(prediction_boston_mar_men_30_40.2_data, n=50)
```

As we can see in this model the results are much more accurate rather than the previous one. 


# Conclusion

We have seen that there are several techniques to analyze different set of data, since from comparing the sets in order to be sure that they belong to the same distribution
as we have seen at the beginning of the assignment with Chi-Square Test. After that we have been analyzing our distributions and samples with ANOVA in order to see how is the variability of those
populations and see if they belong to the same group or not. Lately with Linear regression models we have seen show different factors are correlated and based on that correlation, if we can build a 
Linear Model in order to predict data on similar populations. 

Regarding Data Analysis we have seen that there is no one powerful technique which explains all the behaviors we are looking for. In fact in the last analysis we have seen that with the help of PCA, 
we could rectify a wrong assumption of the Linear Regression Model.




