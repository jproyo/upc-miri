\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\renewcommand{\algorithmicrequire}{\textbf{Input}}
\renewcommand{\algorithmicensure}{\textbf{Output}}

\theoremstyle{definition}
\newtheorem*{euclidean}{Euclidean Space}
\newtheorem*{hamming}{Hamming metric space}
\newtheorem*{nns}{Nearest Neighbor Search ($\epsilon-NNS$)}

\title{%
      Homework 1 \\
      Locality Sensitivity Hashing
    }
\author{Juan Pablo Royo Sales}
\date\today

\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{}
\fancyhead[R]{Juan Pablo Royo Sales - UPC MIRI}
\fancyhead[L]{ADM - Locality Sensitivity Hashing}
\fancyfoot[L,C]{}
\fancyfoot[R]{Page \thepage{} of \pageref{LastPage}}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\begin{document}

\maketitle

\section{Introduction}
Similarity Search problem are algorithms in which given a collection of objects, we are required to find all those objects that are similar to an specific one. By \textit{similar}, we mean that given an multidimensional point (object) $p_i = (x_1, x_2, \dots x_d)$ where $x_d$ is the $d$-dimension characteristic of $p_i$, there exists a function $f: R^d \to r \mid d > 0, r \in R$ that measure the distance $r$ on each dimension $x_dÂ \land \forall p_j \mid i \neq j$, returning $\argmin_{p_j} = \{ f(p_j) \mid \forall p_j \in R^d \}$ whose distance is the smallest one.

This kind of algorithms are used for different kind of problems: information retrieval, pattern image recognition, text search and everything that can be represented as a \textit{k}-dimensional in some dimensional space $R^d \mid d > 0$. In particular we are going to study in the $d$-dimensional \textit{Euclidean} space.

It is well know that there are several different approaches to solve \textit{Nearest Neighbor} as for example $kd$-trees, but these kind of structures start to perform bad when the amount of dimensions exceeds above \textit{10} or \textit{20} doing it not better than \textit{brute force} linear search.

In this sense, we are going to describe a \textbf{sub-linear} optimized algorithm which is call \textbf{Locality Sensitivity Hashing} \textit{(LSH)}.

The main idea of \textbf{LSH} is to hash the points, in order to have a higher probability of collision of those points which are \textit{similar} or close to each other, rather than those who are apart.

In this document we are going to analyze and describe all the details needed to understand \textit{LSH} based on this paper \cite{gionis_sim_search}.

\section{Definitions}

\begin{euclidean}
  We use $l^{d}_p$ to denote the \textit{Euclidean} space $R^d$ under the $l_p$ norm, i.e.\ when the length of the vector $(x_1, x_2, \dots, x_d)$ is defined as $(|x_1|^p + |x_2|^p + \dots + |x_d|^p)^{1/p}$. Also, $d_p(p,q) = || p - q ||_p$ which indicates the distance between the point $p$ and $q$ in $l^{d}_p$.
\end{euclidean}

\begin{hamming}
  We use $H^d$ to denote \textit{Hamming metric space} of dimension $d$, i.e., the space of binary vectors of length $d$ under the standard \textbf{Hamming} metric. We use $d_H(p,q)$ to denote the \textbf{Hamming distance}, i.e., the number of bits on which $p$ differs from $q$.
\end{hamming}

\begin{nns}
  Given a set $P$ of points in a normed space $l^{d}_p$, preprocess $P$ so as to efficiently return a point $p \in P$ for any given query point $q$, such that $d(q, p) \leq (1 + \epsilon)d(q,P)$ where $d(q, P)$ is the distance of $q$ to the its closest point
in $P$.
\end{nns}

\section{LSH - Algorithm}
This algorithm was first introduced here \cite{motwani_lsh}, but as we mentioned before in the introduction, we are going to be based on the improvements made it in this \cite{gionis_sim_search} work. In their work they have improved original algorithm with the following features:

\begin{itemize}
  \item Hash buckets are not required
  \item It has better running time
  \item The analysis is generalized to the case of secondary memory
\end{itemize}

The algorithm proceed as follows:

\begin{enumerate}
  \item First encode the largest point $C \in P$ into the Hamming cube $H^{d'}$ where $d' = Cd$ transforming each point $p$ into a binary vector $v(p) = Unary(x_1) \dots Unary(x_d)$ where $Unary(x)$ represents the unary representation of $x$. Basically it is a sequence of $x$ ones followed by $C$-zeroes.
    With this encoding we are going to have the maximum possible point $C$ encoding in Hamming cube, plus all the real points encoding in unary representation followed by $0$-zeroes to complete the padding.
  \item Take the distance of each pair of points $p$ and $q$ with coordinates in the set $\{1,\dots, C\}$ such that for :
    \begin{equation*}
      d_1(p,q) = d_H(v(p), v(q))
    \end{equation*}

    This means that the encoding does not affect (or preserve), the distance between points.

\subsection{Preprocessing algorithm}

    \begin{algorithm}
     \label{alg:preproc}
     \caption{Preprocessing}
     \begin{algorithmic}
     \REQUIRE $A$ set of points $P$,
     \STATE $l$ (number of hash tables),
     \ENSURE Hash tables $T_i, i = 1, \dots ,l$
     \FORALL{$i = 1, \dots, l$}
      \STATE Initialize hash table $T_i$ by generating
      a random hash function $g_i()$
     \ENDFOR
     \FORALL{$i = 1, \dots, l$}
       \FORALL{$j= 1, \dots, n$}
         \STATE Store point $p_j$ on bucket $g_i(p_j)$ of hash table $T_i$
       \ENDFOR
     \ENDFOR
   \end{algorithmic}
   \end{algorithm}

\subsection{$\epsilon-NNS$ Algorithm}

\begin{algorithm}
  \label{algo:nns}
  \caption{Approximate Nearest Neighbor Query}
  \begin{algorithmic}
  \REQUIRE A query point $q$,
  \STATE $K$ (number of appr.\ nearest neighbors)
  \STATE \textbf{Access} To hash tables $T_i, i = 1, \dots, l$ generated by the preprocessing algorithm
  \ENSURE $K$ (or less) appr.\ nearest neighbors
  \STATE $S \leftarrow \emptyset$
  \FORALL{$i = 1, \dots, l$}
    \STATE $S \leftarrow S \cup \{ \text{points found in}\ g_i(q)\ \text{bucket of table }\ T_i\}$
  \ENDFOR
  \RETURN the $K$ nearest neighbors of $q$ found in set $S$
  \STATE \textit{/*Can be found by main memory linear search*/}
  \end{algorithmic}
\end{algorithm}

\end{enumerate}


\begin{thebibliography}{9}
\bibitem{gionis_sim_search}
  Aristedes Gionis, Piotr Indyk, Rajeev Motwani. 1999. \textit{Similarity Search in High Dimensions via Hashing}. In Vldb, Vol. 99. 518-529.
\bibitem{motwani_lsh}
  P. Indyk, R. Motwani. 1998. \textit{Approximate Nearest Neighbor - Towards Removing the Curse of Dimensionality}. In Proceedings of the 30th Symposium on Theory of Computing, 604-613.
\end{thebibliography}

\end{document}

