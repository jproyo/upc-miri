\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{color}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\renewcommand{\algorithmicrequire}{\textbf{Input}}
\renewcommand{\algorithmicensure}{\textbf{Output}}

\theoremstyle{definition}
\newtheorem*{euclidean}{Euclidean Space}
\newtheorem*{hamming}{Hamming metric space}
\newtheorem*{nns}{Nearest Neighbor Search ($\epsilon-NNS$)}

\title{%
      Homework 1 \\
      Locality Sensitivity Hashing
    }
\author{Juan Pablo Royo Sales}
\date\today

\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{}
\fancyhead[R]{Juan Pablo Royo Sales - UPC MIRI}
\fancyhead[L]{ADM - Locality Sensitivity Hashing}
\fancyfoot[L,C]{}
\fancyfoot[R]{Page \thepage{} of \pageref{LastPage}}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\begin{document}

\maketitle

\section{Introduction}
Similarity Search problem are algorithms in which given a collection of objects, we are required to find all those objects that are similar to an specific one. By \textit{similar}, we mean that given an multidimensional point (object) $p_i = (x_1, x_2, \dots x_d)$ where $x_d$ is the $d$-dimension characteristic of $p_i$, there exists a function $f: R^d \to r \mid d > 0, r \in R$ that measure the distance $r$ on each dimension $x_d, \forall p_j | i \neq j$, returning $\argmin_{p_j} = \{ f(p_j) \mid \forall p_j \in R^d \}$ whose distance is the smallest one.

This kind of algorithms are used for different kind of problems: information retrieval, pattern image recognition, text search and everything that can be represented as a \textit{k}-dimensional in some dimensional space $R^d \mid d > 0$. In particular we are going to do this analysis in the $d$-dimensional \textit{Euclidean} space.

It is well know that there are several different approaches to solve \textit{Nearest Neighbor} as for example $kd$-trees, but these kind of structures start to perform bad when the amount of dimensions exceeds above \textit{10} or \textit{20} doing it not better than \textit{brute force} linear search.

In this sense, we are going to describe a \textbf{sub-linear} optimized algorithm which is call \textbf{Locality Sensitivity Hashing} \textit{(LSH)}.

The main idea of \textbf{LSH} is to hash the points, in order to have a higher probability of collision of those points which are \textit{similar} or close to each other, rather than those who are apart.

In this document we are going to analyze and describe all the details needed to understand \textit{LSH} based on this paper \cite{gionis_sim_search}.

\section{Definitions}

\begin{euclidean}
  We use $l^{d}_p$ to denote the \textit{Euclidean} space $R^d$ under the $l_p$ norm, i.e.\ when the length of the vector $(x_1, x_2, \dots, x_d)$ is defined as $(|x_1|^p + |x_2|^p + \dots + |x_d|^p)^{1/p}$. Also, $d_p(p,q) = || p - q ||_p$ which indicates the distance between the point $p$ and $q$ in $l^{d}_p$.
\end{euclidean}

\begin{hamming}
  We use $H^d$ to denote \textit{Hamming metric space} of dimension $d$, i.e., the space of binary vectors of length $d$ under the standard \textbf{Hamming} metric. We use $d_H(p,q)$ to denote the \textbf{Hamming distance}, i.e., the number of bits on which $p$ differs from $q$.
\end{hamming}

\begin{nns}
  Given a set $P$ of points in a normed space $l^{d}_p$, preprocess $P$ so as to efficiently return a point $p \in P$ for any given query point $q$, such that $d(q, p) \leq (1 + \epsilon)d(q,P)$ where $d(q, P)$ is the distance of $q$ to the its closest point
in $P$.
\end{nns}

\section{LSH - Algorithm}
This algorithm was first introduced here \cite{motwani_lsh}, but as we mentioned before in the introduction, we are going to conduct the analysis based on the improvements made it in this \cite{gionis_sim_search} work. In that paper the authors have improved original algorithm with the following features:

\begin{itemize}
  \item Hash buckets are not required
  \item It has better running time
  \item The analysis is generalized to the case of secondary memory
\end{itemize}

The algorithm proceed as follows:

\begin{enumerate}
  \item First encode the largest point $C \in P$ into the Hamming cube $H^{d'}$ where $d' = Cd$ transforming each point $p$ into a binary vector $v(p) = Unary(x_1) \dots Unary(x_d)$ where $Unary(x)$ represents the unary representation of $x$. Basically it is a sequence of $x$ ones followed by $C$-zeroes.
    With this encoding we are going to have the maximum possible point $C$ encoding in Hamming cube, plus all the real points encoding in unary representation followed by $0$-zeroes to complete the padding.
  \item Take the distance of each pair of points $p$ and $q$ with coordinates in the set $\{1,\dots, C\}$ such that for :
    \begin{equation*}
      d_1(p,q) = d_H(v(p), v(q))
    \end{equation*}

    This means that the encoding does not affect (or preserve), the distance between points.
  \item Taking the Hamming distance we are going to compute the Hash functions on each point to construct a Hash tables for each function based on the similarities, providing the fact that the probability of collision is going to increase for similar points.

In the following subsection we are going to describe how the algorithm of building this Hash tables works and how \textbf{NNS} find the nearest points.
\end{enumerate}

\subsection{Preprocessing algorithm}

    \begin{algorithm}[H]
     \label{alg:preproc}
     \caption{Preprocessing}
     \begin{algorithmic}
     \REQUIRE $A$ set of points $P$,
     \STATE $l$ (number of hash tables),
     \ENSURE Hash tables $T_i, i = 1, \dots ,l$
     \FORALL{$i = 1, \dots, l$}
      \STATE Initialize hash table $T_i$ by generating
      a random hash function $g_i()$
     \ENDFOR
     \FORALL{$i = 1, \dots, l$}
       \FORALL{$j= 1, \dots, n$}
         \STATE Store point $p_j$ on bucket $g_i(p_j)$ of hash table $T_i$
       \ENDFOR
     \ENDFOR
   \end{algorithmic}
   \end{algorithm}

   This algorithm is the preprocessing of building all the Hash tables with all the points applying the \textbf{hash} function $g_i$ for each Hash table.

   The hash functions $g_i$ are defined as following:

   \begin{itemize}
     \item For an Integer $l$ to be specified, $l$ subsets from $I_1, \dots, I_l$ of ${1, \dots, d'}$ are chosen.
     \item $p_{|I}$ is the projection of the vector $p$ in the coordinate set $I$. We compute that $p_{|I}$ selection the coordinate positions according to $I$ and adding bits to that position.
     \item Then $g_j(p) = p_{|I_j}$
     \item Store each $p \in P$ in the bucket $g_j(p), \forall j = 1, \dots, l$.
  \end{itemize}

  Since the number of buckets can increase too much, there is a second hashing level which is a standard hash for the points $p$ that fell into the bucket $g_j(p)$

\subsection{Approximate NNS Algorithm}

\begin{algorithm}[H]
  \label{algo:nns}
  \caption{Approximate Nearest Neighbor Query}
  \begin{algorithmic}
  \REQUIRE A query point $q$,
  \STATE $K$ (number of appr.\ nearest neighbors)
  \STATE \textbf{Access} To hash tables $T_i, i = 1, \dots, l$ generated by the preprocessing algorithm
  \ENSURE $K$ (or less) appr.\ nearest neighbors
  \STATE $S \leftarrow \emptyset$
  \FORALL{$i = 1, \dots, l$}
    \STATE $S \leftarrow S \cup \{ \text{points found in}\ g_i(q)\ \text{bucket of table }\ T_i\}$
  \ENDFOR
  \RETURN the $K$ nearest neighbors of $q$ found in set $S$
  \STATE \textit{/*Can be found by main memory linear search*/}
  \end{algorithmic}
\end{algorithm}

In this algorithm we are finally doing the approximate search, where giving one query point $q$, we are iterating over all $l$ points that we use on previous $g_j(p)$ hash functions to found all the points that are on the buckets that match with $q$ into a resulting subset $S$.


\section{Analysis of LSH}
As we describe in the introduction section, the main idea behind this method rest on the fact that the Probability of collisions regarding hashing, which increase if the elements are near each other.

This statement is formalized in the following way:

\newtheorem*{prob}{Definition}
\begin{prob}{Definition}
  Let $D(.,.)$ be the Hamming distance function of elements from a set $S$
  Let $B(p,r)$ be the set of elements from $S$ within distance from $r$ to $p$
  A family $H$ of functions from $S$ to $U$ is called $(r_1, r_2, p_1, p_2)$-sensitive for $D(.,.)$ if for any $q, p \in S$
  \begin{itemize}
    \item if $p \in B(q,r_1) \implies P[h(q) = h(p)] \geq p_1$
    \item if $p \notin B(q,r_2) \implies P[h(q) = h(p)] \leq p_2$
  \end{itemize}
  Such that $p_1 > p_2$ and $r_1 < r_2$
\end{prob}

What the first implication of the definition is saying is that the probability of collision in the hashing function of 2 points increase when the 2 points are similar. We can notice that because $p \in B(q,r_1)$, so $p$ it is within a close distance with regards of the subset of points between $q$ and $r_1$. In the second implication the probability of collision in hashes decreases when $p$ it is not in the subset of distance points.

In this analysis we are generalizing the algorithm to an arbitrary local-sensitive family of Hash functions $H$, instead of only one. So our $g_i$ function has the following form:

\begin{equation*}
  g_i(p) = (h_{i1}(p), h_{i2}(p), \dots, h_{ik}(p))
\end{equation*}

Where the hashing functions $h_{ik}$ are u.a.r.\ choosen with replacement.

There are two things that this algorithm state in the cited paper \cite{gionis_sim_search}, one is that $whp$ we can assure that hashing collision takes place when the elements are near, and the other is that the complexity of the algorithm is sub linear.

We are going to give a brief analysis now on the authors works that prove this 2 strong statements.

\subsection{High Probability of Hashing Collisions for Near elements}
The prove that we can have high collision with high probability for nearest elements is done assuming that there are 2 properties that holds in order to achieve this:

\begin{enumerate}
 \item \textbf{P1} If there exists a $p^*$ such that $p^* \in B(q, r_1) \implies g_j(p^*) = g_j(p)$ for some $j = 1, \dots, l$. This means that if the are an element that is near, if we can find other near they have the same output in the hashes functions.
 \item \textbf{P2} The total number of blocks pointed by $q$ and containing only points from $P'$ is less than $cl$.
\end{enumerate}

Assuming that $H$ is a $(r_1, r_2, p_1, p_2)$-sensitive; it is defined that $\rho = \frac{\ln{1/p_1}}{\ln{1/p_2}}$. The correctness of \textbf{LSH} algorithm follows the following theorem:

\newtheorem{high}{Theorem 1}
\begin{high}
  Setting $k = \log_{1/p_2}(n/B)$ and $l = (\frac{n}{B})^\rho$ guarantees that properties \textbf{P1} and \textbf{P2} holds with high probability at least $\frac{1}{2} - \frac{1}{\epsilon} \geq 0.132$
\end{high}

This is achieve by amplifying the algorithm on $O(1/\delta)$ times for obtaining a high probability.

\subsection{Sub Linear complexity}
Basically the prove is done assuming that $r < \frac{d'}{\ln{n}}$. This can be easily done by augmenting the number of dimensions, and that is trivial by padding with $0$-zeroes at the end of each point representation on Hamming encoding.

Assuming that we have that:

\begin{subequations}
  \begin{align}
  \rho &= \frac{\ln{1/p_1}}{\ln{1/p_2}} \\
       &= \frac{\ln{\frac{1}{1-r/{d'}}}}{\ln{\frac{1}{1-(1+\epsilon)r/{d'}}}} \\
       &= \frac{\ln{(1-r/{d'})}}{\ln{(1-(1+\epsilon)r/{d'})}}
  \end{align}
\end{subequations}

After this we multiply numerator and denominator by $d'/r$ and we obtain:

\begin{equation*}
  \frac{U}{L} = \frac{\ln{(1-r/{d'})^{d'/r}}}{\ln{(1-(1+\epsilon)r/{d'})^{d'/r}}}
\end{equation*}

In order to upper bound $\rho$, we need to bound $U$ from below and $L$ from above; note that both $U$ and $L$ are negative. So, we end up with the following equation:

\begin{equation*}
\frac{U}{L} < \frac{1}{1+\epsilon} - \ln{(1-\frac{1}{\ln n})}
\end{equation*}


Having that $\epsilon > 0$ and $r < \frac{d'}{\ln{n}}$, it can be conclude that:


\begin{subequations}
  \begin{align}
    n^{\rho} &< n^{1/(1+\epsilon)} n^{- \ln{(1-1/\ln n)}} \\
             &= n^{1/(1+\epsilon)} (1-1/\ln n)^{-\ln n} \\
             &= O(n^{1/(1+\epsilon)})
  \end{align}
\end{subequations}



\section{Conclusion}
We have seen that \textit{Locality Sensitivity Hashing} algorithm is a powerful technique for doing Similarity Search with sub linear complexity.
In further works we can set up an experiment similar to the author's work in order to compare the performance and results between this algorithm and other multidimensional data structure, \textbf{kd-trees}, which in the paper is mention but it perform bad above 10 dimensions.




\begin{thebibliography}{9}
\bibitem{gionis_sim_search}
  Aristedes Gionis, Piotr Indyk, Rajeev Motwani. 1999. \textit{Similarity Search in High Dimensions via Hashing}. In Vldb, Vol. 99. 518-529.
\bibitem{motwani_lsh}
  P. Indyk, R. Motwani. 1998. \textit{Approximate Nearest Neighbor - Towards Removing the Curse of Dimensionality}. In Proceedings of the 30th Symposium on Theory of Computing, 604-613.
\end{thebibliography}

\end{document}

